---
title:  "[NPU] Cambricon ISA"
excerpt: ""

categories:
  - Architecture
tags:
  - [NPU]

breadcrumb: true
toc: true
toc_sticky: true
 
date: 2023-01-09
last_modified_at: 2023-01-09
use_math: true
---
# Cambricon: An Instruction Set Architecture for Neural Networks

# 0. Abstract

Neural Networks(NN)ì€ ê´‘ë²”ìœ„í•œ ìƒˆë¡œìš´ machine leanringê³¼ íŒ¨í„´ ì¸ì‹ applicationë“¤ì„ ìœ„í•œ ëª¨ë¸ ì§‘í•©ì´ë‹¤. 

NN í…Œí¬ë‹‰ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ general-purpose processors(cpu, gpgpu)ì—ì„œ ì‹¤í–‰ë˜ëŠ”ë°, ì´ê²ƒë“¤ì€ ë‹¤ì–‘í•œ workloadë¥¼ ìœ ì—°í•˜ê²Œ ì§€ì›í•˜ê¸° ìœ„í•´ hardware resourceë“¤ì„ ì§€ë‚˜ì¹˜ê²Œ íˆ¬ìí•  ë•Œ ë³´í†µ ì—ë„ˆì§€ íš¨ìœ¨ì ì´ì§€ ì•Šë‹¤.

ë”°ë¼ì„œ, NNì„ ìœ„í•œ application-specificí•œ hardware acceleratorë“¤ì´ ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ìµœê·¼ì— ì œì•ˆëë‹¤.

ê·¸ëŸ¬ë‚˜ ê·¸ëŸ° accleratorë“¤ì€ ìœ ì‚¬í•œ ê³„ì‚° íŒ¨í„´ì„ ê³µìœ í•˜ëŠ” NN í…Œí¬ë‹‰ë“¤ì˜ ê·¹íˆ ì¼ë¶€ë¶„ì„ ìœ„í•´ ë””ìì¸ë˜ì—ˆê³ , ê·¸ë“¤ì€ NNì˜ ê³ ìˆ˜ì¤€ functional block(layer) ë˜ëŠ” ì‹¬ì§€ì–´ NN ì „ì²´ì— ì§ì ‘ ëŒ€ì‘ë˜ëŠ” ë³µì¡í•˜ê³  ì •ë³´ëŸ‰ì´ ë§ì€ instructionë“¤ì„ ì±„íƒí•˜ì˜€ë‹¤.

ë¹„ë¡ ë¹„ìŠ·í•œ NN í…Œí¬ë‹‰ì—ì„œì˜ ì œí•œëœ setì—ì„œ ê°„ë‹¨í•˜ê³  êµ¬í˜„í•˜ê¸° ì‰½ì§€ë§Œ, instruction set ì—ì„œì˜ ë¯¼ì²©ì„± ë¶€ì¡±ì€ ë‹¤ì–‘í•œ NN í…Œí¬ë‹‰ì˜ íš¨ìœ¨ì„±ê³¼ ì¶©ë¶„í•œ ìœ ì—°ì„±ì„ ì§€ì›í•˜ê¸° ìœ„í•œ accelerators ë””ìì¸ì„ ë§Œë“¤ê¸° ì–´ë µê²Œ í•œë‹¤.

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì¡´ì¬í•˜ëŠ” NN techniqueë“¤ì˜ í¬ê´„ì ì¸ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ scalar,vector,matrix,logical,data transfer, control instructionì„ í†µí•©í•˜ê³  load-store architectureë¥¼ ì‚¬ìš©í•œ **Cambriconì´ë¼ê³  ë¶ˆë¦¬ëŠ” NN acceleratorë“¤ì˜ ìƒˆë¡œìš´ domain-specific ISAë¥¼ ì œì•ˆí•œë‹¤.**

cambriconì— ë‹¤ì–‘í•œ NN techniqueë“¤ì— ëŒ€í•´ì„œ ìˆ˜ìš©ë ¥ì´ ìˆë‹¤ëŠ” ê²ƒì„ 10ê°œê°€ ë„˜ëŠ” ëŒ€í‘œì ì¸ ë„˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ NN techniqueë“¤ì˜ ì‹¤í—˜ì„ í†µí•´ ì¦ëª…í•˜ê³  MIPS, GPGPU, x86ê³¼ ê°™ì€ general-purpose ISAë³´ë‹¤ ë†’ì€ code densityë¥¼ ì œê³µí•˜ëŠ” ê²ƒë„ ë³´ì¸ë‹¤.

ìµœê·¼ DaDianNao(3ê°€ì§€ NN í…Œí¬ë‹‰ì„ ì§€ì›)ê³¼ Cambricon-based acceleratorì™€ ë¹„êµí•œë‹¤.

> **Cambricon ISAëŠ” ë‹¤ë¥¸ NN acceleratorë“¤ì˜ ISAì™€ëŠ” ë‹¤ë¥´ê²Œ ë‹¤ì–‘í•œ NN í…Œí¬ë‹‰ë“¤ì„ ì§€ì›í•˜ê³ , ê¸°ì¡´ ë²”ìš© ISAë³´ë‹¤ ë†’ì€ code densityë¥¼ ì œê³µí•œë‹¤(ì„±ëŠ¥ì´ ì¢‹ë‹¤)**
> 

# 1. Introduction

NNì€ ë‡Œê³¼í•™ì—ì„œ ì˜ê°ì„ ì–»ì–´ ì‹œì‘ëœ ì•„ì£¼ í° ë²”ìœ„ì˜ machine learning ê¸°ìˆ ì´ê³  ìµœê·¼ 10ë…„ë„˜ê²Œ ê¹Šê³  í¬ê²Œ ë°œì „í•´ì™”ë‹¤.

ë¹„ë¡ NNì´ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ì§€ë§Œ, ê·¸ ì¤‘ ì¼ë¶€ëŠ” íŠ¹ì • ì‘ì—…ì—ì„œ ì¸ê°„ì˜ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.

ì›ë˜ **NN í…Œí¬ë‹‰ì€ CPUì™€ GPGPUë¡œ êµ¬ì„±ëœ gerneral purpose platformìœ¼ë¡œ ì‹¤í–‰**ë˜ì—ˆë‹¤. 

ì´ **CPUì™€ GPGPU** ëª¨ë‘ ë‹¤ì–‘í•œ workloadë¥¼ ìœ ì—°í•˜ê²Œ ì§€ì›í•˜ê¸° ìœ„í•´ hardware resourceë¥¼ ê³¼ë„í•˜ê²Œ íˆ¬ìí•˜ê¸° ë•Œë¬¸ì— **ì—ë„ˆì§€ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§„ë‹¤.** 

ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ìµœê·¼ NNì— ë§ì¶¤í™”ëœ hardware acceleratorë“¤ì´ ì—°êµ¬ë˜ê³  ìˆë‹¤.

ì´ëŸ¬í•œ hardware accleratorë“¤ì€ ë‚®ì€ ìˆ˜ì¤€ì˜ ì‘ì—…(dot-product)ì´ ì•„ë‹Œ ë†’ì€ ìˆ˜ì¤€ì˜ layer(convolution, pooling, classifier) ë˜ëŠ” NNì „ì²´ì— í•´ë‹¹í•˜ëŠ” ê³ ìˆ˜ì¤€ ê·¸ë¦¬ê³  infomative instructionë“¤ì„ ë„ì…í•˜ì˜€ê³ , ê·¸ë“¤ì˜ decoderë“¤ì€ ê° instructionì— fully optimized ë  ìˆ˜ ìˆë‹¤.

ë¹„ë¡ ìœ ì‚¬í•œ NN ê¸°ìˆ ì˜ ì‘ì€ set(small instruction set)ì— ëŒ€í•´ ê°„ë‹¨í•˜ê³  êµ¬í˜„í•˜ê¸° ì‰½ì§€ë§Œ, ë‹¤ì–‘í•œ NN ê¸°ìˆ ì„ ìœ ì—°í•˜ê²Œ ì§€ì›í•´ì•¼ í•  í•„ìš”ì„±ì´ instruction setì˜ ìƒë‹¹í•œ í™•ì¥ì„ ì´ˆë˜í•  ë•Œ, ê·¸ëŸ¬í•œ acceleratorì— ëŒ€í•œ ë””ì½”ë”ì˜ ì„¤ê³„ ê²€ì¦ ë³µì¡ì„±ê³¼ ë©´ì /ì „ë ¥ ì˜¤ë²„í—¤ë“œëŠ” ë°›ì•„ë“¤ì¼ ìˆ˜ ì—†ì„ ì •ë„ê¹Œì§€ ì‰½ê²Œ ì»¤ì§„ë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ, ì´ëŸ¬í•œ accleratorë“¤ì€ ë§¤ìš° ìœ ì‚¬í•œ ê³„ì‚° íŒ¨í„´ê³¼ data localityë¥¼ ê³µìœ í•˜ëŠ” NN í…Œí¬ë‹‰ì˜ ì‘ì€ subsetì—ì„œë§Œ íš¨ìœ¨ì ìœ¼ë¡œ ì§€ì›í•  ìˆ˜ ìˆê³ , ê¸°ì¡´ NN ê¸°ìˆ ê°„ì˜ ë‹¤ì–‘ì„±ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ë‹¤.

ì˜ˆë¥¼ë“¤ì–´ SOTA NNê°€ì†ê¸°ì¸ DadiaNaoëŠ” MLPë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ, ë‰´ëŸ°ë“¤ì´ ê°ê° fully connected ëœ BMì€ ìˆ˜ìš©í•  ìˆ˜ ì—†ë‹¤.

*ê²°ê³¼ì ìœ¼ë¡œ ISA ì„¤ê³„ëŠ” í˜„ì¡´í•˜ëŠ” NN accleratorë“¤ì˜ **ìœ ì—°ì„±**ê³¼ **íš¨ìœ¨ì„±**ì„ í¬ê²Œ ì œí•œí•˜ëŠ” ì§€ê¸ˆê¹Œì§€ ë¯¸í•´ê²°ëœ ê·¼ë³¸ì ì¸ ê³¼ì œì´ë‹¤.*

> **ë²”ìš© ISA(CPU, GPGPU)ëŠ” ì—ë„ˆì§€ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì ¸ì„œ NN acceleratorë“¤ì´ ë‚˜ì™”ì§€ë§Œ, ìœ ì—°ì„±ê³¼ íš¨ìœ¨ì„±ì´ ë¶€ì¡±í•˜ë‹¤.**
> 

ì´ ë…¼ë¬¸ì—ì„œëŠ” RISC ISA ì„¤ê³„ ì›ë¦¬ì— ì˜ê°ì„ ë°›ì•„ NN acceleratorì˜ ISAë¥¼ ì„¤ê³„í•˜ì˜€ë‹¤.

NNì˜ ê³ ìˆ˜ì¤€ functional block(layer)ë“¤ì„ ì„¤ëª…í•˜ëŠ” ë³µì¡í•˜ê³  ì •ë³´ê°€ ë§ì€ instructionë“¤ì„ ì €ìˆ˜ì¤€ ê³„ì‚° ì‘ì—…(dot product)ì— í•´ë‹¹í•˜ëŠ” ì§§ì€ instructionìœ¼ë¡œ ë¶„ë¦¬í•˜ë©´ ì‚¬ìš©ìê°€ ì´ì œ ì €ìˆ˜ì¤€ì˜ ì‘ì—…ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ NN í…Œí¬ë‹‰ì— í•„ìˆ˜ì ì¸ ìƒˆë¡œìš´ ê³ ìˆ˜ì¤€ì˜ functional blockì„ ì¡°í•©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°€ì†ê¸°ê°€ ë” ë„“ì€ ì‘ìš© ë²”ìœ„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.

> **ê¸°ì¡´ì—ëŠ” ê³ ìˆ˜ì¤€ì— ë§ëŠ” instructionë“¤ì„ ë„ì…í•˜ì˜€ì§€ë§Œ, ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ISAëŠ” dot productì™€ ê°™ì€ ì €ìˆ˜ì¤€ì˜ instructionì„ ì‚¬ìš©í•˜ì—¬ ê³ ìˆ˜ì¤€ì˜ blockì„ ì¡°í•©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì—°ì„±ì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤.**
> 

ì´ë ‡ê²Œ ì„¤ê³„í•œ ISAë¥¼ **Cambricon**ìœ¼ë¡œ ëª…ëª…í•˜ì˜€ë‹¤. Cambriconì€ instructionì€ ëª¨ë‘ 64bitì¸ load-store ì•„í‚¤í…ì³ë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , ì£¼ë¡œ control ë° address ì§€ì • ëª©ì ì„ ìœ„í•´ ìŠ¤ì¹¼ë¼ë¥¼ ìœ„í•œ 64 32bitì˜ General-Purpose Registers(GPRs)ë¥¼  ì‚¬ìš©í•˜ì˜€ë‹¤.

<aside>
ğŸ’¡ ì™œ control ë° address ì§€ì •ì„ í•˜ë ¤ê³  32, 64bitì˜ GPRsë¥¼ ì‚¬ìš©í• ê¹Œ?
â†’ register-indirect addressingì„ ìœ„í•´ì„œ 32,64ì˜ GPRsë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ë¦¬ê³  scalar dataì˜ ì„ì‹œë³´ê´€ì´ ê°€ëŠ¥í•˜ë‹¤.

</aside>

ì˜ì—­, ì „ë ¥ ì˜¤ë²„í—¤ë“œê°€ ê±°ì˜ ì—†ëŠ” vertor, matrix dataì— ëŒ€í•œ **ì§‘ì¤‘ì ì´ê³  ì§€ì†ì ì¸** ê°€ë³€ ê¸¸ì´ì˜ ì ‘ê·¼ì„ ì§€ì›í•˜ê¸° ìœ„í•´ì„œ, Cambriconì€ vector register fileì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  dataë¥¼ í”„ë¡œê·¸ë˜ë¨¸ì™€ ì»´íŒŒì¼ëŸ¬ì—ê²Œ visibleí•œ on-chip scratchpad memoryì— ì €ì¥í•œë‹¤. 

ì£¼ì†Œì˜ í•˜ìœ„ ë¹„íŠ¸ë¡œ ë¶„í•´ëœ ì„œë¡œ ë‹¤ë¥¸ ë±…í¬ì— ëŒ€í•œ ë™ì‹œ ì•¡ì„¸ìŠ¤ëŠ” NN ê¸°ìˆ ì„ ì§€ì›í•˜ê¸°ì— ì¶©ë¶„í•˜ê¸° ë•Œë¬¸ì— on-chip ë©”ëª¨ë¦¬ì— ì—¬ëŸ¬ í¬íŠ¸ë¥¼ êµ¬í˜„í•  í•„ìš”ê°€ ì—†ë‹¤.

register fileì˜ ì œí•œëœ í­ì— ì˜í•´ ì„±ëŠ¥ì´ ì œí•œë˜ëŠ” SIMDì™€ ë‹¬ë¦¬ ìº ë¸Œë¦¬ì½˜ì€ on-chip scratchpad memoryì˜ ë±…í¬ë¥¼ register fileë³´ë‹¤ ì‰½ê²Œ ë„“í ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë” í¬ê³  ê°€ë³€ì ì¸ ë°ì´í„° widthë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì§€ì›í•œë‹¤.

> **vector, matrix dataì— ëŒ€í•œ ì§€ì†ì ì¸ ê°€ë³€ì ì¸ ê¸¸ì´ì˜ ì ‘ê·¼ì„ ìœ„í•´ dataë¥¼ register fileì— ì €ì¥í•˜ì§€ ì•Šê³ , register fileë³´ë‹¤ ë±…í¬ë¥¼ ì‰½ê²Œ ë„“í ìˆ˜ ìˆëŠ” on-chip scratchpad memoryì— ì €ì¥í•œë‹¤.**
> 

<aside>
ğŸ’¡ NN íŠ¹ì„±ìƒ matrix dataëŠ” ê¸¸ì´ê°€ ì •í•´ì ¸ìˆì§€ ì•Šì•„ì„œ ê°€ë³€ê¸¸ì´ì˜ ì ‘ê·¼ì„±ì´ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°

</aside>

10ê°œì˜ ëŒ€í‘œì ì¸ NN í…Œí¬ë‹‰(MLP, CNN, RNN, LSTM, AE, SAE, BM, RBM, SOM, HNN)ìœ¼ë¡œ Cambriconì„ í‰ê°€í•˜ì˜€ë‹¤. ê·¸ ê²°ê³¼ MIPSë³´ë‹¤ 13.9ë°°, x86ë³´ë‹¤ 9.86ë°°, GPGPUë³´ë‹¤ 6.41ë°°ì˜ code density, ìµœì‹  NN accerlerator ì„¤ê³„ì¸ DaDianNaoì™€ ë¹„êµí•˜ë©´ ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” latency, power, area overhead(4.5%, 4.4%, 1.6%)ê°€ ë°œìƒí•œë‹¤. 

> ë²”ìš© ISAë³´ë‹¤ í›¨ì”¬ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³ , NN accelerator DaDianNaoì™€ëŠ” ê±°ì˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ DaDianNaoëŠ” 3ê°€ì§€ NN í…Œí¬ë‹‰ë§Œì„ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— Cambriconì´ ë” ìœ ì—°í•˜ë‹¤.
> 

ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ì˜ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

1. ë‹¤ì–‘í•œ NN í…Œí¬ë‹‰ë“¤ì— ëŒ€í•´ ìˆ˜ìš©ë ¥ì„ ê°€ì§„ ìƒˆë¡­ê³  ê°€ë²¼ìš´ ISAë¥¼ ì œì•ˆí•œë‹¤.
2. ê¸°ì¡´ NN ê¸°ìˆ  ì—°êµ¬ì˜ ê³„ì‚° íŒ¨í„´ì— ëŒ€í•œ í¬ê´„ì ì¸ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•œë‹¤.
3. TSMC 65nm ê¸°ìˆ ë¡œ êµ¬í˜„í•œë‹¤.

ì´ ë…¼ë¬¸ì˜ ë‚˜ë¨¸ì§€ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

Section 2 : Cambriconì„ ë”°ë¥´ëŠ” ì„¤ê³„ ê°€ì´ë“œë¼ì¸ê³¼ overviewë¥¼ ì œì‹œ

Section 3 : Cambriconì˜ computational and logical instructionì„ ì†Œê°œ

Section 4 : Cambricon acceleratorì˜ í”„ë¡œí† íƒ€ì…

Section 5 : ë‹¤ë¥¸ ISAë“¤ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ  

Section 6 : Cambriconì˜ í™•ì¥ ì ì¬ë ¥ì— ëŒ€í•œ ë…¼ì˜

# 2. Overview of The Proposed ISA

ì´ ì„¹ì…˜ì—ì„œëŠ” ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ISAì— ëŒ€í•œ ì„¤ê³„ ê°€ì´ë“œë¼ì¸ ë° ISA ê°œìš”ì— ëŒ€í•´ ì„¤ëª…í•œë‹¤.

## A. Design Guidelines

NNì„ ìœ„í•œ ê°„ê²°, ìœ ì—°, íš¨ìœ¨ì ì¸ ISAë¥¼ ì„¤ê³„í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” êµ¬ì²´ì ì¸ ì„¤ê³„ ê²°ì •ì„ ë‚´ë¦¬ê¸° ì „ì— ëª‡ ê°€ì§€ ì„¤ê³„ ê°€ì´ë“œë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ NN í…Œí¬ë‹‰ë“¤ì„ ê·¸ë“¤ì˜ ê³„ì‚° operationê³¼ ë©”ëª¨ë¦¬ ì•¡ì„¸ìŠ¤ íŒ¨í„´ì˜ ì¸¡ë©´ì—ì„œ ë¶„ì„í•˜ì˜€ë‹¤.

### Data-level Parallelism(DLP)

ëŒ€ë¶€ë¶„ì˜ NN í…Œí¬ë‹‰ë“¤ì´ ë‰´ëŸ°ê³¼ ì‹œëƒ…ìŠ¤ ë°ì´í„°ë“¤ì´ layerë“¤ë¡œ êµ¬ì„±ëœ ë‹¤ìŒ ê· ì¼í•˜ê³  ëŒ€ì¹­ì ì¸ ë°©ì‹ìœ¼ë¡œ ì¡°ì‘ëœë‹¤ëŠ” ê²ƒì„ ì•Œì•„ëƒˆë‹¤. 

ì´ëŸ¬í•œ ì—°ì‚°ì„ ì´ìš©í•  ë•Œ vector/matrix instructionë“¤ë¡œ ì§€ì›ë˜ëŠ” DLPê°€ ì „í†µì ì¸ scalar instructionì˜ ILPë³´ë‹¤ ë” íš¨ìœ¨ì ì´ê³ , ë” ë†’ì€ ì½”ë“œ ë°€ë„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.

### Customized Vector/Matrix Instructions

ë§ì€ ì„ í˜•ëŒ€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬(e.g., BLAS ë¼ì´ë¸ŒëŸ¬ë¦¬)ê°€ ì¡´ì¬í•˜ì§€ë§Œ, NN í…Œí¬ë‹‰ì˜ ê²½ìš° ì„ í˜•ëŒ€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì •ì˜ëœ ê¸°ë³¸ operationë“¤ì´ ê¼­ íš¨ê³¼ì ì´ê³  íš¨ìœ¨ì ì¸ ì„ íƒì€ ì•„ë‹ˆë‹¤. 

ë” ì¤‘ìš”í•œ ì ì€, ë§ì€ ê³µí†µì ì¸ NN í…Œí¬ë‹‰ì˜ operationë“¤ì€ ì „í†µì ì¸ ì„ í˜•ëŒ€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë‹¤ë£¨ì§€ ì•ŠëŠ” ê²ƒë“¤ì´ ë§ë‹¤.

ì˜ˆë¥¼ë“¤ë©´ BLAS ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë²¡í„°ì˜ ìš”ì†Œë³„ ì§€ìˆ˜ ê³„ì‚°ì„ ì§€ì›í•˜ì§€ ì•Šê³ , ì‹œëƒ…ìŠ¤ ì´ˆê¸°í™”, ë“œë¡­ì•„ì›ƒ, Restricted Boltzmann Machine(RBM)ì˜ random vector ìƒì„±ë„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

ë”°ë¼ì„œ ê¸°ì¡´ ì„ í˜• ëŒ€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë‹¨ìˆœíˆ vector/matrix ì—°ì‚°ì„ ë‹¤ì‹œ êµ¬í˜„í•˜ëŠ” ëŒ€ì‹  ê¸°ì¡´ NN ê¸°ìˆ ì— ëŒ€í•´ ì‘ì§€ë§Œ ëŒ€í‘œì ì¸ vector/matrix instructionì˜ setì„ ì² ì €í•˜ê²Œ customizeí•´ì•¼ë§Œ í•œë‹¤.

### Using On-chip Scratchpad Memory

NN í…Œí¬ë‹‰ë“¤ì´ vector/matrix ë°ì´í„°ì— ëŒ€í•œ ì§‘ì¤‘ì ì´ê³  ì—°ì†ì ì¸ ê°€ë³€ ê¸¸ì´ì˜ accessê°€ ìì£¼ ìš”êµ¬ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì˜€ë‹¤. 

ê·¸ë ‡ê¸°ì— ê³ ì • ê¸¸ì´ì˜ vector register fileë“¤ì€ ë”ì´ìƒ cost-effectiveí•œ ì„ íƒì´ ì•„ë‹ˆê²Œ ëë‹¤.

ìš°ë¦¬ì˜ ì„¤ê³„ì—ì„œ ì´ vector register fileë“¤ì„ ê° ë°ì´í„° accessì— ìœ ì—°í•œ ê¸¸ì´ë¥¼ ì œê³µí•˜ëŠ” on-chip scratchpad memoryë¡œ ëŒ€ì²´í•˜ì˜€ë‹¤.

> **Data-level Parallelism(DLP)ë¥¼ ì‚¬ìš©í•˜ê³ , ê¸°ì¡´ ì„ í˜•ëŒ€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  custom í•œ instruction setì„ ì‚¬ìš©í•˜ê³ , on-chip scratchpad memoryë¥¼ ì‚¬ìš©í•œë‹¤.**
> 

## B. An Overview to Cambricon

Cambriconì€ load/store instructionìœ¼ë¡œ main memoryì— accessí•˜ê²Œ í—ˆìš©í•˜ëŠ” load-store ì•„í‚¤í…ì³ì´ë‹¤. 

Cambriconì€ scalarsë¥¼ ìœ„í•œ 64 32bitì˜ GPRsë¥¼ í¬í•¨í•œë‹¤. ì´ëŠ” on-chip scratchpad memoryì˜ **register-indirect addressing**ì— ì‚¬ìš©ë  ë¿ë§Œ ì•„ë‹ˆë¼ scalar dataë¥¼ ì„ì‹œë¡œ ë³´ê´€í•˜ëŠ” ìš©ë„ë¡œë„ ì‚¬ìš©ëœë‹¤.

### Type of Instructions

Cambriconì€ 4ê°€ì§€ íƒ€ì…ì˜ instructionì„ í¬í•¨í•œë‹¤. 

1. computational
2. logical
3. control
4. data transfer

ë¹„ë¡ ë‹¤ë¥¸ instructionë“¤ë¼ë¦¬ëŠ” valid bitë“¤ì˜ ìˆ˜ê°€ ë‹¤ë¥´ê² ì§€ë§Œ, instructionì˜ ê¸¸ì´ëŠ” **memory-alignment**ë¥¼ ìœ„í•˜ì—¬, ë˜ load/store/decoding ê³¼ì •ì„ ê°„ë‹¨í•˜ê²Œ ì„¤ê³„í•˜ê¸° ìœ„í•´ 64bitë¡œ ê³ ì •ë˜ì–´ìˆë‹¤. 

ì´ ì„¹ì…˜ì—ì„œ control(jump, branch)ê³¼ data transfer(load/store/move) instructionì— ëŒ€í•´ì„œëŠ” ë¹„ë¡ NN í…Œí¬ë‹‰ì— ë§ê²Œ ì¡°ì •ë˜ì—ˆì§€ë§Œ MIPS instructionë“¤ê³¼ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì— ê°„ê²°í•˜ê²Œ ì„¤ëª…í•œë‹¤. 

computational instructionê³¼ logical instructionë“¤(matrix, vector, scalar instructionì„ í¬í•¨í•˜ëŠ”)ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒì„¹ì…˜(3ì¥)ì—ì„œ ì„¤ëª…í•œë‹¤.

### Control Instructions

ìº ë¸Œë¦¬ì½˜ì—ëŠ” 2ê°€ì§€ control instructionì¸ jump ì™€ conditional branchê°€ ì¡´ì¬í•œë‹¤.

jump instructionì€ PCì— ë”í•´ì§ˆ ê°’ì¸ immediateê°’ ë˜ëŠ” GPR ê°’ì„ í†µí•´ offsetì„ ì§€ì •í•œë‹¤.

conditional branchëŠ” 0ê³¼ predictor(GPRì— ì €ì¥ë˜ì–´ìˆëŠ”)ë¥¼ ë¹„êµí•˜ì—¬, branchì˜ íƒ€ê²Ÿì„ PC+1ë¡œ ì •í• ì§€, PC+{offset}ìœ¼ë¡œ í• ì§€ ê²°ì •í•œë‹¤. 

![https://user-images.githubusercontent.com/41818011/211181448-d4f44ee4-c9d9-4213-8921-a6378e69c412.png](https://user-images.githubusercontent.com/41818011/211181448-d4f44ee4-c9d9-4213-8921-a6378e69c412.png)

<aside>
ğŸ’¡ MIPS64ì—ë„ instructionê¸¸ì´ëŠ” 32ë¹„íŠ¸ë¼ ìœ„ì™€ ê°™ì€ í˜•íƒœê°€ ì•„ë‹ˆë‹¤. 64bitì¸ instructionì„ ì‚¬ìš©í•˜ëŠ” ISAì˜ êµ¬ì¡°ë¥¼ í•œë²ˆ ì‚´í´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤.

</aside>

### Data Transfer Instructinos

Cambriconì˜ Data transfer instructionì€ ìœ ì—°í•˜ê²Œ matrix, vector computational/logical instructionì„ ì§€ì›í•˜ê¸° ìœ„í•´ì„œ ê°€ë³€ ê¸¸ì´ì˜ dataë¥¼ ì§€ì›í•œë‹¤.

ì´ instructionë“¤ì€ main memory â†’ on-chip scratchpad, on-chip scratchpad â†’ scalar GPRs variable-size data blockì„ load/store í•  ìˆ˜ ìˆë‹¤.

VLOAD(vector load) instructionì€ $V_{size}$ í¬ê¸° ë§Œí¼ì˜ vectorë¥¼ main memoryì—ì„œ vector scratchpad memoryë¡œ ë¶ˆëŸ¬ì˜¨ë‹¤.

main memoryì•ˆì˜ source addressëŠ” GPRì— ì €ì¥ë˜ì–´ìˆëŠ” base addressì™€  immediate ê°’ì„ ë”í•˜ë©´ ëœë‹¤.

VSTORE, MLOAD, MSTORE ëª¨ë‘ VLOADì™€ ìœ ì‚¬í•˜ë‹¤.

![https://user-images.githubusercontent.com/41818011/211187910-d55bb962-2cee-443d-b9b4-806a018a9625.png](https://user-images.githubusercontent.com/41818011/211187910-d55bb962-2cee-443d-b9b4-806a018a9625.png)

### On-chip Scratchpad Memory

Cambriconì€ vector register fileì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëŒ€ì‹  programmer/compilerì—ê²Œ visibleí•˜ê²Œ ë§Œë“¤ì–´ì§„ on-chip scratchpadì— ë°ì´í„°ë¥¼ ì§ì ‘ ë³´ê´€í•œë‹¤.

ë‹¤ì‹œë§í•´, ì „í†µì ì¸ ISAë“¤ì˜ vector register fileê³¼ Cambriconì˜ scratchpad memoryì˜ ì—­í• ì´ ë¹„ìŠ·í•˜ê³ , vector í”¼ì—°ì‚°ìì˜ í¬ê¸°ê°€ ë” ì´ìƒ ê³ ì •ê¸¸ì´ vector register fileì— ì˜í•´ ì œí•œë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤.

ê·¸ëŸ¬ë¯€ë¡œ, vector/matrix í¬ê¸°ëŠ” Cambricon instructionì— ì˜í•´ ë³€í•˜ê²Œ ë˜ê³ , ë‹¤ë§Œ í•œê°€ì§€ ì£¼ëª©í• ë§Œí•œ ì œí•œì ì€ ë™ì¼í•œ instruction ë‚´ë¶€ì˜ vector/matrix í”¼ì—°ì‚°ìê°€ scratchpad memoryì˜ ìš©ëŸ‰ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ë‹¤ëŠ” ì ì´ë‹¤.

ë§Œì•½ ì´ˆê³¼í•œë‹¤ë©´, compilerëŠ” ê¸´ vector/matrixë¥¼ ì§§ì€ ì¡°ê° ë° ë¸”ë¡ìœ¼ë¡œ ë¶„í•´í•˜ê³  ì—¬ëŸ¬ê°œì˜ instructionì„ ë§Œë“¤ì–´ ì²˜ë¦¬í•œë‹¤. 

32x512b ë²¡í„° ë ˆì§€ìŠ¤í„°ê°€ Intel AVX-512ì— êµ¬ì›Œì§„ ê²ƒ ì²˜ëŸ¼ vector/matrix instructionì„ ìœ„í•œ on-chip memoryì˜ ìš©ëŸ‰ë„ ê³ ì •ë˜ì–´ì•¼ í•œë‹¤.

ë” ìƒì„¸í•˜ê²Œ ë§í•®ë©´, Cambriconì˜ memory ìš©ëŸ‰ì€ vector instructionì— ëŒ€í•´ì„œëŠ” 64KBë¡œ ê³ ì •ë˜ê³ , matrix instructionì— ëŒ€í•´ì„œëŠ” 768KBë¡œ ê³ ì •ëœë‹¤.

ê·¸ë ‡ì§€ë§Œ, Cambriconì€ scratchpad memoryì˜ bank ê°¯ìˆ˜ì— ëŒ€í•´ì„œëŠ” íŠ¹ë³„í•œ ì œí•œì„ ë‘ì§€ ì•Šì•„ì„œ, microarchitecture-level êµ¬í˜„ì— ìƒë‹¹í•œ ììœ ë¥¼ ì¤€ë‹¤.

> **Cambriconì€ 4ê°€ì§€ íƒ€ì…ì˜ instructionìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆê³ , ì´ ì¤‘ controlê³¼ data transfer instructionì€ ê¸°ì¡´ MIPSì˜ êµ¬ì¡°ì™€ ë¹„ìŠ·í•˜ë‹¤. ë˜í•œ, on-chip scratchpad memoryë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ê°€ë³€ê¸¸ì´ì—ì„œì˜ ì¥ì ì´ ìˆë‹¤.**
> 

# 3. Computational/Logical instructions

NNì—ì„œ ëŒ€ë¶€ë¶„ì˜ operation(addition, multiplication, activation functionë“¤)ì€ vector operationìœ¼ë¡œ ëª¨ì—¬ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ê·¸ ë¹„ìœ¨ì€ 99.992%ê¹Œì§€ ë†’ì•„ ì§ˆ ìˆ˜ ìˆë‹¤.

ì¦‰, NNì€ scalar, vector, matrix ì—°ì‚°ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í•´ë  ìˆ˜ ìˆìœ¼ë©°, ISAëŠ” DLPì™€ data localityë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì–´ì•¼ í•œë‹¤.

## A. Matrix Instructions

ì¡´ì¬í•˜ëŠ” NN í…Œí¬ë‹‰ë“¤ì— ëŒ€í•´ ì² ì €í•˜ê³  í¬ê´„ì ì¸ ë¶„ì„ì„ í•˜ì˜€ê³ , Cambriconì„ ìœ„í•œ ì´ 6ê°œì˜ matrix instructionì„ ì„¤ê³„í•˜ì˜€ë‹¤.

NNì¤‘ MLPë¥¼ ì‚¬ìš©í•˜ì—¬ matrix instructionì´ ì–´ë–»ê²Œ ì§€ì›ë˜ëŠ”ì§€ ë³´ì—¬ì¤„ ê²ƒì´ë‹¤.

ê¸°ìˆ ì ìœ¼ë¡œ MLPëŠ” ë³´í†µ multiple layerë“¤ì„ ê°€ì§€ê³ , ê° layerëŠ” Input ë‰´ëŸ°ì— ë”°ë¼ output ë‰´ëŸ°ì„ ê³„ì‚°í•œë‹¤.

![https://user-images.githubusercontent.com/41818011/211195924-5d7a6978-b793-4289-95c5-407ed0b6e380.png](https://user-images.githubusercontent.com/41818011/211195924-5d7a6978-b793-4289-95c5-407ed0b6e380.png)

output ë‰´ëŸ°ì¸ $y_i\space(i = 1,2,3)$ ì€ $y_i = f(\Sigma_{j=1}^{3} w_{ij}x_{j}+b_i)$ì™€ ê°™ì´ ê³„ì‚°ëœë‹¤.

$x_j$ëŠ” input ë‰´ëŸ°

$w_{ij}$ëŠ” ië²ˆì§¸ output ë‰´ëŸ°ê³¼ jë²ˆì§¸ input ë‰´ëŸ°ì˜ weight

$b_i$ëŠ” ië²ˆì§¸ ì•„ì›ƒí’‹ ë‰´ëŸ°ì˜ bias

$f$ëŠ” activation function

ê·¸ë˜ì„œ output ë‰´ëŸ°ì€ vector $y = (y_1, y_2, y_3)$ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$
y = f(Wx+b)
$$

$x = (x_1, x_2, x_3)$ì´ê³ , $b = (b_1, b_2, b_3)$ì¸ input ë‰´ëŸ°ê³¼ bias vectorì´ë‹¤.

$W = (w_{ij})$ì¸ weight matrixì´ê³ , $f$ëŠ” activation functionì˜ ìš”ì†Œë³„ ë²„ì „ì´ë‹¤.

ìœ„ì˜ ì‹ì—ì„œ $Wx$ë¥¼ ê³„ì‚°í•  ë•Œ, Cambriconì˜ **Matrix-Mult-Vector(MMV)**ì— ì˜í•´ ìˆ˜í–‰ëœë‹¤.

MMV instructionì€ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

![https://user-images.githubusercontent.com/41818011/211196515-fab27f62-d82b-43f7-be5d-9e74c1b5e503.png](https://user-images.githubusercontent.com/41818011/211196515-fab27f62-d82b-43f7-be5d-9e74c1b5e503.png)

Reg0 : vector outputì˜ scratchpad memory base address

Reg1 : vector output size

Reg2 : matrix inputì˜ base address

Reg3 : vector inputì˜ base address

Reg4 : vector inputì˜ size

â†’ matrixì— vectorë¥¼ ê³±í•˜ë©´ vector í˜•íƒœë¡œ ë‚˜ì˜¤ë¯€ë¡œ vectorëŠ” input, outputì´ ëª¨ë‘ ì¡´ì¬í•˜ê³ , matrixëŠ” Inputë§Œ ì¡´ì¬í•œë‹¤.

<aside>
ğŸ’¡ Matrix inputì˜ sizeê°€ ì—†ëŠ” ì´ìœ ëŠ” vector output, inputì— ì˜í•´ ì•Œì•„ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì¸ê°€?

</aside>

MMV instructionì€ ëª¨ë“  input, output ë°ì´í„°ë¥¼ scratchpad memoryì— ë™ì‹œì— ë³´ê´€í•  ìˆ˜ ìˆëŠ” í•œ ì„ì˜ì˜ scaleì—ì„œ matrix-vector ê³±ì…ˆì„ ì§€ì›í•  ìˆ˜ ìˆë‹¤.

$Wx$ë¥¼ ì—¬ëŸ¬ê°œì˜ vector dot ì—°ì‚°ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ëŒ€ì‹ , MMV instructionì„ ì‚¬ìš©í•´ì„œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì—¬ëŸ¬ê°œì˜ vector dot ì—°ì‚°ìœ¼ë¡œ ë°”ê¾¸ì–´ë²„ë¦¬ë©´ input vector $x$ë¥¼ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ë…¸ë ¥ì´ í•„ìš”í•˜ë‹¤. (explicit synchronization, ê°™ì€ addressë¥¼ ìš”ì²­í•˜ì˜€ì„ ë•Œ ë™ì‹œ ì½ê³  ì“°ê¸° ë¬¸ì œ) 

ìœ„ì™€ ê°™ì€ feedforwardì™€ëŠ” ë‹¤ë¥´ê²Œ, MMV instructionì´ ë”ì´ìƒ NNì˜ backforward trainingì—ëŠ” íš¨ìœ¨ì ìœ¼ë¡œ ì œê³µë˜ì§€ ì•ŠëŠ”ë‹¤.  

ë” ìì„¸í•˜ê²ŒëŠ” Back-Propagation(BP)ì•Œê³ ë¦¬ì¦˜ì€ gradient vectorë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.

ì´ê²ƒì„ MMV instructionìœ¼ë¡œ êµ¬í˜„í•˜ë ¤ë©´ ì¶”ê°€ì ìœ¼ë¡œ matrix transposeë¥¼ êµ¬í˜„í•˜ëŠ” instructionì´ í•„ìš”í•˜ë‹¤. ê·¸ë¦¬ê³  ì´ê²ƒì€ ë°ì´í„° íë¦„ì—ì„œ ë” ë§ì€ ë¹„ìš©ì„ í•„ìš”ë¡œ í•œë‹¤.

ì´ê²ƒì„ í”¼í•˜ê¸° ìœ„í•´ì„œ, Cambriconì€ backforward trainingì— ì ìš©ë˜ëŠ” Vector-Mult-Matrix(VMM) instructionì„ ì œê³µí•œë‹¤. VMM instructionì€ opcodeë¥¼ ì œì™¸í•˜ê³  MMVì™€ ë¹„ìŠ·í•œ filedë“¤ì„ ì œê³µí•œë‹¤.

ê²Œë‹¤ê°€ NN training ê³¼ì •ì—ì„œ weight matrix $W$ëŠ” $W = W + \eta\Delta W$ë¡œ ì—…ë°ì´íŠ¸ ëœë‹¤.

ì—¬ê¸°ì„œ $\eta$ëŠ” learning rate(ìŠ¤ì¹¼ë¼)ì´ê³ , $\Delta W$ëŠ” ë‘ vectorì˜ outer productë¡œ ê³„ì‚°ëœë‹¤.

ê·¸ë ‡ê¸°ì— Cambriconì€ outputì´ matrixì¸ Outer-Product(OP) instructionê³¼ Matrix-Mult-Scalar(MMS) instruction, Matrix-Add-Matrix(MAM) instructionì„ ëª¨ë‘ weight updateë¥¼ ìˆ˜í–‰í•  ë•Œ ì œê³µí•œë‹¤.

ê·¸ë¦¬ê³  RBMì—ì„œ weight updateë¥¼ ìœ„í•œ Matrix-Subtract-Matrix(MSM) instruction ë˜í•œ ì œê³µëœë‹¤.

> **NN í…Œí¬ë‹‰ì—ì„œ matrixì— ê´€ë ¨ëœ ì—°ì‚°ì„ ë§ì´ ìˆ˜í–‰í•˜ëŠ”ë°, ê·¸ê²ƒì„ ì§€ì›í•˜ê¸° ìœ„í•´ MMV instructionì„ ì œê³µí•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ forward ê³¼ì •ì—ì„œ ìˆ˜í–‰ë˜ëŠ” ê²ƒì´ê³ , backpropagation ê³¼ì •ì„ ìœ„í•´ VMM, MMS, MAM, MSM, OP ë“±ì˜ instructionë„ ì§€ì›í•œë‹¤.**
> 

## B. Vector instruction

matrix instruction ë§Œìœ¼ë¡œëŠ” ëª¨ë“  ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê¸°ì— ë¶ˆì¶©ë¶„í•˜ë‹¤. $Wx + b$ì™€ ê°™ì€ bias vectorë¥¼ ë”í•˜ëŠ” ì—°ì‚°ì„ í•˜ì˜€ì„ ë•Œ, vector add instructionì´ í•„ìš”í•˜ë‹¤.

Cambriconì€ Vector-Add-Vecvor(VAV) instructionì„ vector additionì„ ìœ„í•´ ì œê³µí•˜ì§€ë§Œ, ìš”ì†Œë³„ activationì„ ì§€ì›í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ê°œì˜ instructionì´ í•„ìš”í•˜ë‹¤.

ì¼ë°˜ì„±ì„ ìƒì§€ ì•Šê¸° ìœ„í•´ ì•„ì£¼ ë„“ê²Œ ì“°ì´ëŠ” sigmoid activation functionì¸ $f(a) = e^a/(1+e^a)$ë¥¼ ì˜ˆì‹œë¡œ ë“¤ì—ˆë‹¤.

 input vector ì˜ ê° ìš”ì†Œì— ëŒ€í•´ ìˆ˜í–‰ë˜ëŠ” ìš”ì†Œë³„ sigmoid activationì€ 3ê°€ì§€ ì—°ì†ì ì¸ stepìœ¼ë¡œ ë¶„í•´ë˜ê³ , ê°ê° 3ê°€ì§€ instructionì— ì˜í•´ ì§€ì›ëœë‹¤.

1. ì¸í’‹ ë²¡í„° aì— ëŒ€í•´ ê° element($a_i,i=1,\dots,n$ )ì˜ exponential $e^{ai}$ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ Cambriconì€ ë²¡í„°ì˜ ìš”ì†Œë³„ exponentialì„ ìœ„í•œ Vector-Exponential(VEXP) instructionì„ ì œê³µí•œë‹¤.
2. ìƒìˆ˜ 1ê³¼ ê° ë²¡í„°ì˜ ìš”ì†Œë¥¼ ë”í•˜ê¸° ìœ„í•´($e^{a1},\dots,e^{a_n}$) Cambriconì€ Vector-Add-Scalar(VAS) instructionì„ ì œê³µí•œë‹¤.  ì´ ë•Œ, ScalarëŠ” immediate ë˜ëŠ” GPRì— ì˜í•´ íŠ¹ì •ë˜ëŠ” ê°’ì´ ëœë‹¤.
3. $e^{a_i}$ë¥¼ $1+e^{a_i}$ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•´, Cambriconì€ ì„œë¡œ ë‹¤ë¥¸ ë²¡í„°ì—ì„œ ìš”ì†Œë³„ divisionì„ ìœ„í•œ Vector-Div-Vector(VDV) instructionì„ ì œê³µí•œë‹¤.

ê·¸ëŸ¬ë‚˜, sigmoidëŠ” NNì˜ ìœ ì¼í•œ activation functionì´ ì•„ë‹ˆë¯€ë¡œ, ë‹¤ì–‘í•œ activation functionì˜ ìš”ì†Œë³„ ë²„ì „ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´, Cambriconì€ Vector-Mult-Vector(VMV), Vector-Sub-Vector(VSV), Vector-Logarithm(VLOG)ì„ ì œê³µí•œë‹¤.

â†’ ì´ëŠ” vector arithmetic instructionì´ë‹¤.

hardware accelerator ì„¤ê³„ ì¤‘ì— ì„œë¡œ ë‹¤ë¥¸ ì´ˆì›”í•¨ìˆ˜(e.g., ë¡œê·¸, ì‚¼ê°í•¨ìˆ˜ ë° ë°˜ì‚¼ê°í•¨ìˆ˜)ì™€ ê´€ë ¨ëœ instructionì€ CORDIC ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ functional block(ì¶”ê°€, ì‹œí”„íŠ¸ ë° í…Œì´ë¸” ë£©ì—… ì—°ì‚° í¬í•¨)ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

ë˜í•œ ë…¼ë¦¬ ì—°ì‚°(e.g., ë¹„êµ)ì— ë¶€ë¶„ì ìœ¼ë¡œ ì˜ì¡´í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜(ì˜ˆ: max(0,a) ë° |a|)ê°€ ìˆìœ¼ë©°, ì„¹ì…˜ III-Cì—ì„œ ê´€ë ¨ Cambricon isntruction(e,g., vector compare instructions)ì„ ì œì‹œí•  ê²ƒì´ë‹¤.

ê²Œë‹¤ê°€, random vector generationì€ ìˆ˜ë§ì€ NN í…Œí¬ë‹‰ë“¤(e.g., dropout, random sampling)ì—ì„œì˜ ì¤‘ìš”í•œ operationì´ë‹¤. ê·¸ëŸ¬ë‚˜ scientific computingì„ ìœ„í•´ ì„¤ê³„ëœ ì„ í˜•ëŒ€ìˆ˜ libraryì—ì„œëŠ” í•„ìˆ˜ê°€ ì•„ë‹ˆë¼ê³  ìƒê°í•œë‹¤.(e.g., BLASëŠ” random vector generationì´ ì—†ë‹¤.)

Cambriconì€ [0,1] ì‚¬ì´ì˜ uniform ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” random numberì˜ vectorë¥¼ ìƒì„±í•˜ëŠ” ì „ìš© instruction(Random-Vector, RV)ì„ ì œê³µí•œë‹¤. 

ì£¼ì–´ì§„ random vectorë“¤ë¡œ vector compare instructionê³¼ vector arithmetic instructionì˜ ë„ì›€ì„ ë°›ì•„ Ziggurat ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ gaussian distributionê³¼ ê°™ì€ ë‹¤ë¥¸ ë¶„í¬ë“¤ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

> **Matrix instruction ë§ê³ ë„ vectorì˜ arithmetic ì—°ì‚°ì„ ìœ„í•´ Vector instructionì´ ì œê³µëœë‹¤.
ê·¸ ì˜ˆë¡œëŠ” VLOG, VEXP, VAV, VDV, VMV, VSV, VAS instructionì´ ìˆë‹¤.
ë˜í•œ vector compare instruction, random-vector instruction ë“±ì„ ì œê³µí•œë‹¤.
ì´ê²ƒë“¤ë¡œ ë§ì€ NN í…Œí¬ë‹‰ì˜ activation function ë° ì—¬ëŸ¬ê°€ì§€ ë¶„í¬ë¥¼ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤.**
> 

## C. Logical Instructions

SOTA NN í…Œí¬ë‹‰ë“¤ì€ logical, comparison ì¡°ì‘ì„ í¬í•¨í•˜ëŠ” ëª‡ê°€ì§€ operationì„ í™œìš©í•œë‹¤. 

max-pooling operationì€ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ pooling window ì•ˆì—ì„œ ê°€ì¥ í° outputì„ ê°€ì§€ëŠ” ë‰´ëŸ°ì„ ì°¾ê³ , ë‹¤ë¥¸ feature mapì—ì„œ ì´ëŸ¬í•œ í–‰ë™ì„ ë°˜ë³µí•˜ëŠ” ì‘ì—…ì´ë‹¤.

![https://user-images.githubusercontent.com/41818011/211204343-caba8825-910c-49a2-9d52-5234ff100d17.png](https://user-images.githubusercontent.com/41818011/211204343-caba8825-910c-49a2-9d52-5234ff100d17.png)

Cambriconì€ Vector-Greater-Than-Merge(VGTM) instructionì„ ì‚¬ìš©í•˜ì—¬ max-pooling operationì„ ì§€ì›í•œë‹¤. ì´ VGTM instructionì˜ êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.